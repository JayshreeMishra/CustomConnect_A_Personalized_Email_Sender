{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Corrector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The public version of the Corpus of Linguistic Acceptability (CoLA) dataset contains 9,594 sentences from training and development sets, which are used to assess the grammatical correctness of sentences. The dataset utilized in this project is derived from the original CoLA dataset, with grammatically correct sentences removed.\n",
    "\n",
    "##### Dataset download links:\n",
    "- in_domain_train.tsv:- https://github.com/nyu-mll/CoLA-baselines/blob/master/acceptability_corpus/cola_public/raw/in_domain_train.tsv\n",
    "- in_domain_dev.tsv:- https://github.com/nyu-mll/CoLA-baselines/blob/master/acceptability_corpus/cola_public/raw/in_domain_dev.tsv\n",
    "\n",
    "This revision clarifies the purpose of the dataset and improves the overall readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from symspellpy import SymSpell\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from language_tool_python import LanguageTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As you eat the most, you want the least.</td>\n",
       "      <td>As you eat more, you desire less.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The more you would want, the less you would eat.</td>\n",
       "      <td>The more you desire, the less you eat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I demand that the more John eat, the more he p...</td>\n",
       "      <td>I demand that the more John eats, the more he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The more does Bill smoke, the more Susan hates...</td>\n",
       "      <td>The more Bill smokes, the more Susan hates him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who does John visit Sally because he likes?</td>\n",
       "      <td>Whom does John visit because he likes Sally?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0           As you eat the most, you want the least.   \n",
       "1   The more you would want, the less you would eat.   \n",
       "2  I demand that the more John eat, the more he p...   \n",
       "3  The more does Bill smoke, the more Susan hates...   \n",
       "4        Who does John visit Sally because he likes?   \n",
       "\n",
       "                                         target_text  \n",
       "0                As you eat more, you desire less.    \n",
       "1           The more you desire, the less you eat.    \n",
       "2  I demand that the more John eats, the more he ...  \n",
       "3  The more Bill smokes, the more Susan hates him.    \n",
       "4     Whom does John visit because he likes Sally?    "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'../data/grammar_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=r\"..\\data\\grammar_data.csv\")\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 2)\n",
      "(104, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary used by symspell is a made from combining the following files.\n",
    "- core-wordnet.txt: https://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt\n",
    "- en-80k.txt: blob:https://github.com/f5083720-193f-45aa-8469-2b47b81d9314\n",
    "- morphosemantic-links.xls: https://wordnetcode.princeton.edu/standoff-files/morphosemantic-links.xls\n",
    "- teleological-links.xls: https://wordnetcode.princeton.edu/standoff-files/teleological-links.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dictionary saved as 'combined-dictionary.txt'\n"
     ]
    }
   ],
   "source": [
    "en_80k = pd.read_csv(r\"../data/en-80k.txt\", sep=\"\\t\", header=None, names=[\"word1\", \"frequency\"])\n",
    "core_wordnet = pd.read_csv(r\"../data/core-wordnet.txt\", sep=\"\\t\", header=None, names=[\"word1\", \"relation\", \"word2\", \"gloss1\", \"gloss2\"])\n",
    "teleological = pd.read_csv(r\"../data/teleological-links.txt\", sep=\"\\t\", header=None, names=[\"word1\", \"relation\", \"word2\"])\n",
    "morphosemantic = pd.read_csv(r\"../data/morphosemantic-links.txt\", sep=\"\\t\", header=None, names=[\"word1\", \"relation\", \"word2\", \"gloss1\", \"gloss2\"])\n",
    "\n",
    "teleological[\"gloss1\"] = \"\"\n",
    "teleological[\"gloss2\"] = \"\"\n",
    "\n",
    "en_80k[\"relation\"] = \"frequency\"\n",
    "en_80k[\"word2\"] = \"\"\n",
    "en_80k[\"gloss1\"] = \"\"\n",
    "en_80k[\"gloss2\"] = \"\"\n",
    "\n",
    "# Combine all datasets\n",
    "combined = pd.concat([teleological, morphosemantic, core_wordnet, en_80k], ignore_index=True)\n",
    "\n",
    "# Save the combined file\n",
    "combined.to_csv(r\"../data/combined-dictionary.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Combined dictionary saved as 'combined-dictionary.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "dictionary_path = r\"../data/combined-dictionary.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(texts, sym_spell):\n",
    "    corrected_texts = []\n",
    "    for text in texts:\n",
    "        suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "        corrected_texts.append(suggestions[0].term if suggestions else text)\n",
    "    return corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LanguageTool for grammar correction (if needed)\n",
    "tool = LanguageTool('en')\n",
    "\n",
    "def correct_grammar(texts):\n",
    "    corrected_texts = []\n",
    "    for text in texts:\n",
    "        matches = tool.check(text)\n",
    "        corrected_text = tool.correct(text)\n",
    "        corrected_texts.append(corrected_text)\n",
    "    return corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and tokenize the dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    targets = tokenizer(examples['target_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Adjust labels for training (replace padding token ID with -100)\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
    "        for labels in targets[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    # Add labels to inputs for training\n",
    "    inputs[\"labels\"] = labels\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying preprocessing\n",
    "decoded_refs = []\n",
    "for label in tokenized_test_dataset['labels']:\n",
    "    filtered_label = [token if token >= 0 else tokenizer.pad_token_id for token in label]\n",
    "    decoded_refs.append(tokenizer.decode(filtered_label, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spelling correction\n",
    "input_texts = test_dataset['input_text']  # Extract input texts from test dataset\n",
    "spelling_corrected = correct_spelling(input_texts, sym_spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grammar correction\n",
    "final_corrected = correct_grammar(spelling_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Prediction: Bill pushed harry off the sofa for hours\n",
      "Reference: Bill pushed Harry off the sofa repeatedly for hours.  \n",
      "\n",
      "Corrected Prediction: Sharon came the room\n",
      "Reference: Sharon entered the room.  \n",
      "\n",
      "Corrected Prediction: The bottle drained the liquid free\n",
      "Reference: The bottle was drained of its liquid.  \n",
      "\n",
      "Corrected Prediction: Sam gave the ball out of the basket\n",
      "Reference: Sam took the ball out of the basket.  \n",
      "\n",
      "Corrected Prediction: The more pictures of himself appear in the news the more likely john is to get arrested\n",
      "Reference: The more pictures of himself that appear in the news, the more likely John is to get arrested.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for corrected_pred, ref in zip(final_corrected[:5], test_dataset['target_text'][:5]):\n",
    "    print(f\"Corrected Prediction: {corrected_pred}\")\n",
    "    print(f\"Reference: {ref}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
